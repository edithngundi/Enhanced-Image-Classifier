{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Racial Recognition in Image Classification through Enhanced Ensemble Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "> In this mini-project, my focus is on developing an advanced image classification system using ensemble training. This marks the second iteration of creating an image classifier, and the primary goal remains unchanged – evaluating the model's effectiveness in recognizing individuals of the same racial background.\n",
    ">\n",
    "> The project aims to elevate a Convolutional Neural Network (CNN) through ensemble training, taking inspiration from the ensemble trees concept in XGBoost. This strategy involves merging two or more models, each with varying accuracy, to construct a composite model with enhanced performance. While there's no assurance of surpassing every individual model in the ensemble, the goal is to harness the strengths of each architecture, resulting in a model with commendable metric evaluation and versatile task performance. \n",
    ">\n",
    "> In the previous iteration, I developed a basic image classifier that fell short in performance due to the simplicity of the CNN architecture and the limitations of a small dataset comprising only 530 images. In this paper, I will make a comparative performance analysis of the previous simple CNN, a CNN trained on DenseNet169, a CNN trained on ResNet152, a CNN trained on VGG19, and the ensemble model. The models use a larger dataset of 2,000 images and have a more intricate architecture. \n",
    ">\n",
    "> The dataset is sourced from my Google photos and enriched by contributions from the 4 individuals who willingly participated. Data augmentation techniques were applied to expand the dataset. As emphasized in my earlier mini-project, the individuals providing data have granted exclusive permission solely for the purposes of this assignment. I have upheld transparency by informing them that only the write-up and code will be uploaded to my GitHub repository. They are comfortable with their names being included in the write-up but have explicitly requested that only their facial features be utilized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "> Data augmentation is performed using the Augmentor library on images belonging to 4 individuals: Edith, Noni, Aroma, and Macbeth. The original images for each person are loaded using the `get_images` function, and an Augmentor Pipeline is created for each individual's directory. Various techniques are applied, including horizontal flipping, rotation, skewing, and zooming, with specific probability and factor parameters. The `sample` method is then utilized to generate a specified number of augmented images for each individual.\n",
    ">\n",
    "> Additionally, a utility function `rename_files` is defined to rename the augmented images in each directory to ensure sequential numbering. This function takes a starting value and the path to the directory containing the files as parameters, sorts the files to maintain consistency, and iterates through them, renaming each file with a padded sequential number.\n",
    ">\n",
    "> Each individuals's augmented images and original images are then saved in new folders - `EdithImages`, `NoniImages`, `AromaImages`, and `MacbethImages`. The renaming process is applied to maintain a sequential order for easy reference and organization in subsequent steps of the image classification process. `Edith's` images are named 0001 to 2500, `Noni's` are named 2501 to 5000, `Aroma's` are named 5001 to 7500, and `Macbeth's` are named 7501 to 10000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "> After augmenting the dataset using various techniques, a crucial step involves cleaning the data prior to exploration and conversion. Locally, the images from `EdithImages`, `NoniImages`, `AromaImages`, and `MacbethImages` are consolidated into a folder named `FacesImages`. The `get_images` function systematically adds image paths to a list, while the `get_labels` function associates each image with a corresponding label. The earlier executed sequential naming during augmentation simplifies label assignment, with Edith's images (0001 to 2500) assigned the label 0, Noni's (2501 to 5000) assigned 1, Aroma's (5001 to 7500) assigned 2, and Macbeth's (7501 to 10000) assigned 3. The labels are converted to integers (0, 1, 2, and 3) to align with the model and loss function requirements, which operate on integer labels. The `create_dataframe` function creates a dataframe for subsequent data exploration. These steps play a vital role in ensuring the augmented dataset is properly organized before further conversion and model implementation.\n",
    ">\n",
    "> The original plan involved using the complete dataset of 10,000 images for each model's training. However, during the initial trial, training the classifier with the DenseNet169 pre-trained model on 7 epochs took an extended 3 hours, and accuracy computation on the testing set failed due to a kernel crash. Despite several attempts to adjust RAM and kernel settings, the ResNet152 and VGG19 pre-trained models consistently crashed the kernel when downloading trainable parameters, preventing them from completing the 1st epoch. Faced with these challenges, the approach was adjusted, and the models were subsequently trained with 2000 images each from the original dataset. Each model utilized a distinct set of 2000 images: `shorter_dataset1` for DenseNet169, `shorter_dataset2` for ResNet152, `shorter_dataset3` for VGG19, `shorter_dataset4` for the ensemble model, and `shorter_dataset5` for the simple CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analysis\n",
    "> The data exploration techniques used are random image display and class distribution visualizations. The `display_images` function enables a qualitative analysis by randomly selecting images and presenting them with corresponding labels in organized subplots. This approach offers insights into the dataset's diversity and characteristics regarding the 4 individuals. \n",
    ">\n",
    "> It is a valuable assessment of the dataset's image quality and clarity given that most of the images are augmented. This evaluation is crucial as the effectiveness of a machine learning model can be profoundly influenced by the quality of its input data. Additionally, it serves to confirm that the assigned labels accurately correspond to the content depicted in the images. \n",
    ">\n",
    "> In the second section, the bar graph and pie chart provide absolute counts and proportions for each class, revealing insights into category sizes. Through data augmentation, I ensured the classes were balanced to enhance model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Conversion and Pre-processing\n",
    "\n",
    "> The `preprocess_images` function systematically reads and resizes images from the directory using OpenCV and generates a list of preprocessed images. This step is crucial because it ensures uniformity in image dimensions for effective model training. Subsequently, the `normalize_images` function transforms this list of preprocessed images into a NumPy array, ensuring pixel values are normalized. \n",
    ">\n",
    "> While the pre-trained models are originally trained on the `imagenet` dataset, the images underwent normalization using custom mean and standard deviation specific to each dataset. The imagenet `mean` and `standard deviation` values are `[0.485, 0.456, 0.406]` and `[0.229, 0.224, 0.225]`, respectively. As a general guideline, if the dataset being utilized shares characteristics with ImageNet, using its mean and standard deviation is recommended. However, for datasets with different characteristics, such as the five smaller datasets in this mini-project, custom mean and std calculations were conducted independently for optimal normalization.\n",
    ">\n",
    "> While this reduces the hue and color contrast on the images, specifically in the Predictions section, normalization ensures pixel values are standardized which prevents feature dominance. It also ensures faster model convergence during backpropagation. By dividing each pixel value by 255, the function ensures appropriate scaling for input into various image classification models, enhancing the overall readiness of the dataset for machine learning tasks. Additionally, this conversion is the preferred data format for the libraries and models in use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Classification\n",
    "> The data splitting methods employed here are consistent with those utilized in the initial pipeline.\n",
    ">\n",
    "> The image classification task will be performed using a `Convolutional Neural Network (CNN)` that implements a simple architecture. CNNs are well-suited for image classification tasks as they can automatically learn hierarchical representations of features from the input images. \n",
    ">\n",
    "> The dataset has been divided into `training` and `testing` sets using the `80-20` split. The training set (80%) is used to train the model, and the testing set (20%) is reserved for evaluating the model's performance on unseen data. This helps ensure that the model generalizes well to new, unseen examples.\n",
    ">\n",
    "> Within the `training` set, an additional `80-20` split has been performed to create a `validation` set. This is a strategy to monitor the model's performance during training and prevent overfitting. The validation set is not used for training but is crucial for tuning hyperparameters and assessing the model's performance on data it has not seen during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Simple Convolutional Neural Network\n",
    "\n",
    "> The first pipeline incorporated this model, and the following discussion aims to seamlessly integrate it into a unified paper. The content in this section remains unaltered from the previous iteration.\n",
    ">\n",
    "> Convolutional Neural Network (CNN) consists of multiple layers, each serving a specific purpose in classification. There are two underlying hypotheses that we must assume when building any neural network: \n",
    "> - `Linear independence of the input features`\n",
    "> - `Low dimensionality of the input space`. \n",
    ">\n",
    "> The data typically processed with CNNs doesn’t usually satisfy either of these hypotheses. With the architecture of CNN, neurons of the hidden layers learn possible abstract representations over their input, which typically decrease its dimensionality (Baeldung, 2023). The network then assumes that these abstract representations, and not the underlying input features, are independent of one another. These abstract representations are normally contained in the hidden layer of a CNN and tend to possess a lower dimensionality than that of the input. A CNN thus helps solve the so-called `Curse of Dimensionality` problem, which refers to the exponential increase in the amount of computation required to perform a machine-learning task in relation to the unitary increase in the dimensionality of the input (Baeldung, 2023).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 1. Convolutional layers\n",
    "> To understand the underpinnings of this layer, it's important to look into the `filter` and two critical hyperparameters, `stride` and `padding`.\n",
    "> \n",
    "> The `filter` also known as `kernel` is a small matrix used for the convolution operation. The `convolution operation` involves sliding the filter over the input data, performing element-wise multiplication at each step, and summing the results to produce a single value in the output feature map.\n",
    ">\n",
    "> The dimensions of the filter depend on the channels of the image data points. If they are in `RGB`, then the channels are `3`. Therefore, the filter is a `3x3` matrix.\n",
    ">\n",
    "> A convolutional layer typically uses multiple filters simultaneously. Each filter is responsible for capturing a different feature or pattern in the input data. \n",
    ">\n",
    "> During convolution, the filter slides from left to right and from top to bottom until it passes through the entire input image. The `stride` is the step the filter takes. Performing matrix multiplication with images represented as high-dimension matrices is computationally expensive (Antoniadis, 2023).\n",
    ">\n",
    "> To reduce the load, `downsampling`, which is essentially reduces the spatial dimensions of the image, is often used. This operation is done by setting `stride > 0`. If this is set to `1` for instance, then the filter matrix will skip `1` row and `1` column as it slides over to perform the matrix multiplication. Given that the dataset in this project is comparatively small, downsampling was not performed.\n",
    ">\n",
    "> In the convolutional layer, the pixels located on the corners and the edges are used much less than those in the middle. For instance, if the input image is a `5x5` matrix and the filter is a `3x3` matrix, as the filter slides on the image matrix, the input's central pixel at position `(3,3)` is used `9 times` while the corner pixels `(0,0)`, `(0,5)`, `(5,0)` and `(5,5)` are used `1 time` (Antoniadis, 2023).\n",
    ">\n",
    "> Therefore, the information on the borders of images is not preserved as well as the information in the middle. A simple and powerful solution to this problem is `padding`, which adds rows and columns of zeros to the input image. \n",
    ">\n",
    "> If we apply padding $P$ in an input image of size $W{\\times}H$, the output image has dimensions $(W+2P){\\times}(H+2P)$. This is because the padding is added to the left, right, top and bottom of the input image matrix. In the example with a `5x5` matrix, if padding is set to $P=2$, then the resulting input image matrix is of dimensions `9x9`(Antoniadis, 2023). This is a complex step that I felt inexperienced to look into, so for this model the padding is set to `same` which ensures the output is the same dimension as the input.\n",
    ">\n",
    "> The output of the convolutional layer is an output activation map which is computed using the following inputs (Antoniadis, 2023):\n",
    "> - An input image of dimensions $W_{in} \\times H_{in}$\n",
    "> - A filter of dimensions $k \\times k$\n",
    "> - Stride $s$\n",
    "> - Padding $p$\n",
    ">\n",
    "> The resulting dimensions of the output activation map are:\n",
    "> \n",
    "> $$ W_{out} = \\frac{W_{in} - k + 2p + s} {s} $$\n",
    "> $$ H_{out} = \\frac{H_{in} - k + 2p + s} {s} $$\n",
    ">\n",
    "> This task implements 'same' padding, where the size of the input is kept unchanged after the convolutional layer. So for this case:\n",
    "> $$ W_{out} = W_{in} $$\n",
    "> $$ H_{out} = H_{in} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 2. Pooling layers\n",
    ">\n",
    "> The dimensions of the input data and the parameters of the neural network play a crucial role. So this number can be controlled by the stacking of one or more pooling layers. Depending on the type of the pooling layer, an operation is performed on each channel of the input data independently to summarize its values into a single one and thus keep the most important features (Nanos, 2023).\n",
    ">\n",
    "> The choice to use the max pooling technique was completely arbitrary. This technique first breaks down an input image matrix into submatrices of a specific dimension. For instance, if the input matrix is `4x4`, a suitable breakdown is having `4` submatrices that are of dimension `2x2`. These dimensions `2x2` are know as the `pool size`. I set a similar pool size for my model's max pooling layer.\n",
    ">\n",
    "> Next, the technique chooses the maximum value from each of the 4 submatrices and constructs a `2x2` matrix with these values - hence why it's called max pooling.\n",
    ">\n",
    "> This method maintains the most significant features of the input by reducing its dimensions. The mathematical formula of max pooling is (Nanos, 2023):\n",
    ">\n",
    "> $$ MaxPooling(X)_{i,j,k} = \\underset{\\text{m,n}}{\\text{max}} \\; X_{i \\cdot s_x + m,j \\cdot s_y + n,k}$$\n",
    ">\n",
    "> where $X$ is the input image, $(i,j)$ are the indices of the output, $k$ is the image channel index, $s_x$ and $s_y$ are the stride values in the horizontal and vertical directions, and the and the pooling window is defined by the filter size $f_x$ and $f_y$ centered at the output index $(i,j)$ (Nanos, 2023)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 3. Dropout layer\n",
    ">\n",
    "> The `Dropout layer` is a hidden layer that randomly \"drops out\" (sets to zero) a certain fraction of the neurons during training at each update, effectively making them inactive for that particular iteration. \n",
    ">\n",
    "> A Dropout layer can be applied to the input vector, in which case it nullifies some of its features; but we can also apply it to a hidden layer, in which case it nullifies some hidden neurons (Baeldung, 2023). I implemented the latter for my model.\n",
    "> \n",
    "> Dropout layers are important in training CNNs because they prevent overfitting on the training data. If absent, the first batch of training samples influences the learning in a disproportionately high manner (Baeldung, 2023).\n",
    ">\n",
    "> The choice to have a dropout rate of `0.5` in my model was arbitrary. This means the Dropout hidden layer would randomly drop out 50% of the neurons during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 4. Fully connected layers\n",
    "> \n",
    "> They are of different types, however most of them are where the weights are applied and learned during the training process.\n",
    ">\n",
    "> The `Flatten layer` takes the output of previous layers (convolutional and max-pooling layers) and flattens them into single vectors that will be passed forward as inputs. No weights are assigned here, it is purely used for reshaping the data.\n",
    ">\n",
    "> The `First Fully Connected layer` takes single vector inputs and applies weights to predict the appropriate labels for the images. My model has `128` neurons and uses the `ReLU activation function`. Weights are applied to the flattened input data for each neuron, and the ReLU activation function introduces non-linearity by squishing the linear transformation of each input.\n",
    ">\n",
    "> The computation is as follows:\n",
    ">\n",
    "> $$ Z = X \\cdot W + b $$ \n",
    "> $$ A = ReLU(Z) = max(0,Z) $$\n",
    ">\n",
    "> where $X$ is the input vector, $W$ is the weight vector, $b$ is the bias term, and $Z$ is the linear transformation of the input. The ReLU function replaces any negative values with zero and leaves positive values unchanged - this introduces non-linearity. The linear transformations and activations occur in parallel for each neuron.\n",
    ">\n",
    "> The `Fully Connected Output layer` also known as `Soft-max Activation Layer` gives the final probabilities for each label (Kalita, 2022). In my model, the output is mapped to a probability distribution using the softmax function. The number of neurons here depends on the number of labels in the input data, in this case `4` units. \n",
    ">\n",
    "> Using an arbitrary example, suppose the activation values $(A)$ for the four classes in my model were  $A_0$=1.5, $A_1$=1.3, $A_2$=1.1, $A_3$=1.7, then the softmax activation function will calculate probabilities for each class as follows:\n",
    ">\n",
    "> $$ softmax(A_0) = \\frac {exp(1.5)} {exp(1.5) + exp(1.3) + exp(1.1) + exp(1.7)}$$\n",
    "> $$ softmax(A_1) = \\frac {exp(1.3)} {exp(1.5) + exp(1.3) + exp(1.1) + exp(1.7)}$$\n",
    "> $$ softmax(A_2) = \\frac {exp(1.1)} {exp(1.5) + exp(1.3) + exp(1.1) + exp(1.7)}$$\n",
    "> $$ softmax(A_3) = \\frac {exp(1.7)} {exp(1.5) + exp(1.3) + exp(1.1) + exp(1.7)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Custom Classifier on Pre-Trained Models\n",
    "\n",
    "> Before creating a composite model with the 3 pre-trained pre-trained convolutional neural network (CNN) architectures, it's important to see how each distinct model performs. This will be important when creating the ensemble model since we can choose the best strengths of each specific model and combine them to make a well-performing ensemble model. \n",
    ">\n",
    "> This process involves a comprehensive transfer learning strategy using DenseNet169, ResNet152, and VGG19. Transfer learning is a widely adopted technique in machine learning which leverages knowledge gained from pre-training on large datasets to enhance performance on a specific task. Each of the models in use here undergo a systematic process involving incorporating a custom classifier, loading the models' pre-trained weights from the `imagenet` dataset, freezing their layers, training, and metrics evaluation.\n",
    ">\n",
    "> The layers of the pre-trained model are frozen, preventing them from being updated during training. The choice of a global average pooling layer enhances adaptability to varying input sizes - this was an arbitrary choice. The model is compiled using the `adam optimizer` and `sparse categorical crossentropy loss`, with a `callback` mechanism saving the best weights based on validation loss. The callback ensures that the best-performing model weights are retained, mitigating the risk of overfitting.\n",
    ">\n",
    "> The architectures of the pre-trained models are different, and therefore have different strengths and weaknesses:\n",
    ">\n",
    "> - DenseNet's strength lies in its densely connected blocks, which foster efficient parameter sharing and mitigate the vanishing gradient problem. This architecture often attains high accuracy in image classification tasks. Nevertheless, the trade-off includes a high computational cost and increased memory usage due to the dense connections.\n",
    ">\n",
    "> - ResNet excels in facilitating the training of deep networks, making it also effective in preventing the vanishing gradient problem. However, the resultant increase in depth and parameters contributes to a larger model and increased computational complexity. Notably, ResNet exhibits longer training times compared to the other two models because of the deep network architecture.\n",
    ">\n",
    "> - VGG networks are known for their robust feature extraction capabilities and simple design characterized by repeated convolutional and pooling layers. Despite these advantages, VGG employs smaller filter sizes and stacks multiple layers, potentially intensifying computational requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Custom Classifier on Ensemble Model\n",
    "\n",
    "> This Ensemble Model is designed to enhance performance by combining the strengths of individual models. Each pre-trained model is loaded, their architectures are modified for ensemble compatibility, their layers are frozen and unique names are assigned to each layer for clarity. \n",
    ">\n",
    "> In contrast to typical ensemble architectures, this model is uniquely customized. Each pre-trained model is individually weighted, contributing distinctively to the composite's performance. This approach optimally leverages the strengths of each model in varying proportions. The individual weights are concatenated, normalized, and used to scale the outputs of each model. The final step involves summing these weighted outputs and channeling them through the custom classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparative Performance Analysis\n",
    "\n",
    "> These results show the performance metrics of different convolutional neural network (CNN) architectures, including a simple CNN, DenseNet169, ResNet152, VGG19, and an ensemble model. Each model is evaluated on accuracy, precision, and recall, providing insights into their overall classification capabilities and ability to handle specific aspects of the classification task.\n",
    ">\n",
    ">| Model          | Accuracy | Precision | Recall  |\n",
    ">| -------------- | -------- | --------- | ------- |\n",
    ">| Simple CNN     | 93.00%   | 91.87%    | 91.75%  |\n",
    ">| DenseNet169    | 95.50%   | 85.75%    | 82.75%  |\n",
    ">| ResNet152      | 89.25%   | 46.41%    | 42.00%  |\n",
    ">| VGG19          | 98.50%   | 98.49%    | 98.00%  |\n",
    ">| Ensemble       | 96.00%   | 96.68%    | 94.75%  |\n",
    ">\n",
    "> Starting with the simple CNN, it achieves a commendable accuracy of 93%. The precision and recall scores are also quite balanced, with precision slightly higher than recall, indicating a good ability to correctly classify positive instances while minimizing false positives.\n",
    ">\n",
    "> DenseNet169, a more complex model, outperforms the simple CNN with an accuracy of 95.5%. However, it exhibits a lower precision score of 85.75%, suggesting a relatively higher rate of false positives. The recall score of 82.75% indicates that the model might miss some positive instances. This trade-off between precision and recall highlights the need for a careful balance in the model's decision boundaries.\n",
    ">\n",
    "> ResNet152, while achieving a reasonably high accuracy of 89.25%, shows a noticeable drop in both precision (46.41%) and recall (42%). This suggests that the model struggles with both identifying positive instances accurately and avoiding false positives. The lower precision indicates a high rate of false positives, while the low recall implies that the model might miss a significant number of positive instances.\n",
    ">\n",
    "> VGG19 stands out with an impressive accuracy of 98.5%, showcasing its effectiveness in classification tasks. The precision and recall scores of 98.49% and 98%, respectively, reflect a high level of confidence in both positive and negative predictions. VGG19's strong performance across all metrics suggests its suitability for tasks that demand high precision and recall.\n",
    ">\n",
    "> The ensemble model, combining the strengths of DenseNet169, ResNet152, and VGG19, achieves an overall accuracy of 96%. Notably, the precision score of 96.68% demonstrates the ensemble's ability to make accurate positive predictions, while the recall score of 94.75% indicates a balanced performance in capturing positive instances. The ensemble model's performance highlights the potential advantages of combining diverse models to improve overall classification performance.\n",
    ">\n",
    "> Though the ensemble model's performance is great, it can be further improved by choosing better perfoming base models, optimizing hyperparameters through extensive fine-tuning and using larger datasets. A better adjustment on base model weights can be used to ensure that they contribute unique strengths for effective combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executive Summary\n",
    "> Objective:\n",
    ">\n",
    "> - The objective section outlines the focus of the mini-project, which is the development of an advanced image classification system using ensemble training. The goal is to assess the effectiveness of the model in recognizing individuals of the same racial background. The approach involves elevating a Convolutional Neural Network (CNN) through ensemble training, using models trained on different architectures.\n",
    ">\n",
    "> Data Augmentation:\n",
    ">\n",
    "> - This section describes the data augmentation process applied to images from four individuals. Augmentation techniques such as flipping, rotation, skewing, and zooming were employed. The augmented images were organized into folders, and a renaming process was implemented for easy reference and organization. The dataset was enriched from 530 to 10,000 images and the author ensured transparency and permission from contributors.\n",
    ">\n",
    "> Data Cleaning:\n",
    "> \n",
    "> - After data augmentation, this section explains the process of cleaning the dataset for exploration and conversion. The images were consolidated into a single folder, and functions were created to associate labels and create a dataframe for analysis. Due to challenges with training on the complete dataset, a subset of 2000 images per model was used, addressing computational limitations.\n",
    ">\n",
    "> Exploratory Analysis:\n",
    ">\n",
    "> - The exploratory analysis section discusses the techniques employed, including random image display and class distribution visualizations. The display_images function provides a qualitative analysis, while bar graphs and pie charts present class distribution insights. The focus is on assessing image quality, confirming label accuracy, and ensuring a balanced dataset through augmentation.\n",
    ">\n",
    "> Data Conversion and Pre-processing:\n",
    ">\n",
    "> - This section covers the systematic conversion and pre-processing of images. Functions like preprocess_images and normalize_images were applied to ensure uniformity in dimensions and pixel values. Custom mean and standard deviation calculations were performed for normalization. The resulting dataset, in NumPy array format, is well-prepared for use in various image classification models.\n",
    ">\n",
    "> Data Classification:\n",
    ">\n",
    "> - The data classification section details the splitting of the dataset into training and testing sets, with an additional split for validation. The CNN model used for image classification is introduced, and the rationale behind the 80-20 split is explained. This section lays the groundwork for subsequent model development and evaluation.\n",
    ">\n",
    "> Model 1: Simple Convolutional Neural Network:\n",
    "> \n",
    "> - Describing the architecture and concepts of a simple CNN, this section explains the role of convolutional layers, pooling layers, dropout layers, and fully connected layers. The section provides insights into how the model addresses the challenges of input dimensionality and facilitates feature learning. Mathematical formulations for key operations are presented.\n",
    ">\n",
    "> Model 2: Custom Classifier on Pre-Trained Models:\n",
    ">\n",
    "> - Focusing on transfer learning, this section details the process of incorporating custom classifiers on pre-trained models (DenseNet169, ResNet152, and VGG19). The strengths and weaknesses of each architecture are highlighted, and the freezing of layers during training is explained. The models are systematically compiled, trained, and evaluated.\n",
    ">\n",
    "> Model 3: Custom Classifier on Ensemble Model:\n",
    ">\n",
    "> - This section introduces an ensemble model designed to combine the strengths of individual models. Each pre-trained model undergoes customization, with distinct weights assigned for optimal contribution. The weighted outputs are combined through a custom classifier, aiming to enhance overall performance through effective collaboration.\n",
    ">\n",
    "> Comparative Performance Analysis:\n",
    ">\n",
    "> - The final section presents a comparative analysis of different CNN architectures, including a simple CNN, DenseNet169, ResNet152, VGG19, and an ensemble model. Evaluation metrics such as accuracy, precision, and recall are used to assess their classification capabilities. The strengths and trade-offs of each model are discussed, emphasizing the ensemble model's balanced performance. Opportunities for further improvement are highlighted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of AI Tools\n",
    "For this assignment, I used ChatGPT for a number of tasks:\n",
    "\n",
    "- Creating a breakdown of all the steps taken in building the pre-trained models and the ensemble model \n",
    "- Adding comments and docstrings to new code\n",
    "- Debugging. It was particularly helpful in keeping up with the shapes of the matrices.\n",
    "- Building the ensemble model class. Specifically, the code on updating the weight contributions of each base model.\n",
    "- Proofreading and editing the write-up.\n",
    "- Generating the executive summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "Antoniadis, W. by: P. (2023, April 14). Calculate the output size of a convolutional layer. Baeldung on Computer Science. https://www.baeldung.com/cs/convolutional-layer-size \n",
    "\n",
    "BinaryStudy. (2022, August 15). How to normalize image dataset in pytorch. How to Normalize Image Dataset in PyTorch. https://www.binarystudy.com/2022/04/how-to-normalize-image-dataset-inpytorch.html#:~:text=Normalizing%20the%20image%20dataset%20means,by%20the%20channel%20standard%20deviation.\n",
    "\n",
    "Densenet. PyTorch. (n.d.). https://pytorch.org/hub/pytorch_vision_densenet/ \n",
    "\n",
    "GeeksforGeeks. (2023, August 3). Python: Data augmentation. GeeksforGeeks. https://www.geeksforgeeks.org/python-data-augmentation/ \n",
    "\n",
    "Main features. Main Features - Augmentor 0.2.12 documentation. (n.d.). https://augmentor.readthedocs.io/en/stable/userguide/mainfeatures.html#notes\n",
    " \n",
    "Marcelino, P. (2022, April 30). Transfer learning from pre-trained models. Medium. https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751 \n",
    "\n",
    "P, A. (2022, February 12). How to train an ensemble of convolutional neural networks for Image Classification. Medium. https://medium.com/@alexppppp/how-to-train-an-ensemble-of-convolutional-neural-networks-for-image-classification-8fc69b087d3 \n",
    "\n",
    "Tf.keras.applications.densenet.densenet169&nbsp; :&nbsp;  tensorflow V2.14.0. TensorFlow. (n.d.-a). https://www.tensorflow.org/api_docs/python/tf/keras/applications/densenet/DenseNet169 \n",
    "\n",
    "Tf.keras.layers.dense&nbsp; :&nbsp;  tensorflow V2.14.0. TensorFlow. (n.d.-b). https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
